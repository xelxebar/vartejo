#!/usr/bin/env bash
shopt -s -o errexit nounset pipefail

# MIT License
#
# Copyright (c) 2017 Brandon Wilson
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# A tool for processing securities and equities
#
# Conceptually, this program is a pipeline to process data. The birds-eye view
# of data flow is as follows:
#
#     Fetch Data >> Munge Data >> Visualize Data.
#
# At each stage data is read from stdin, processed and written to stdout.
#
# The ``Fetch Data'' stage is primarily responsible for downloading raw data
# from relevant ``vendors'' and postprocessing that vendor-specific data into
# this program's internal data format:
#
#     Fetch Raw Data >> Postproces.
#
# It's important to note that this step should never result in data-loss.
#
# The internal data format is just CSV without a header line. Metadata is also
# just represented as a CSV header prepended to the data CSV and separated by a
# newline, similar to HTTP, SMTP etc. Visually,
#
#     <header field><sep><header field><sep>...
#
#     <data field><sep><data field><sep>...
#     <data field><sep><data field><sep>...
#     ...
#
# NOTE: CSV is well suited to line-oriented processing; however the format
# becomes ambiguous when the field separator can appear in the data. To
# circumvent this, the field separator is configurable via the
# '${setting[csv-separator]}' variable.
#
# The ``Munge Data'' is where the actual math happens. It expects
# well-formatted data and ``munges'' it by performing whatever calculation is
# of interest. The output should also be a well-formatted CSV. After this step
# the original data will likely be processed away.
#
# Finally, the ``Visualize Data'' step is responsible presenting the processed
# data to the user. This may vary from simple CSV column-selection to complex
# 3D graphs.


### UI Settings
#

readonly -A setting=(
	[plot-vert-padding]=0.1
	[plot-candlestick-neg-color]='red'
	[plot-candlestick-pos-color]='green'
	[plot-dgrid3d-mesh-size]=100
	[ema-weight]=0.8
	[ema-min]=0.0
	[ema-max]=1.0
	[ema-bin-count]=100
	[csv-separator]=$'\t'
)


## Data Representation
#

declare -A _ts  # rhash of 'columns_time_series'
readonly -a columns_time_series=(
	'id'
	'date'
	'open'
	'high'
	'low'
	'close'
	'volume'
)

declare -A _me  # rhash of 'columns_multi_ema'
readonly -a columns_multi_ema=(
	'id'
	'weight'
	'ema'
)

declare -A _sl  # rhash of 'columns_securities_list'
readonly -a columns_securities_list=(
	'id'
	'symbol'
	'name'
	'sector'
	'last-sale'
	'cap'
)


## Internal Globals
#

readonly -A error=(
	[bad-args]=101
	[bad-cmd]=102
	[stdin-empty]=201
)


### Helper Functions
#

## Fail with optional error message on stderr
#
# @param [1]  Return code
# @param [2]  Error message
fail()
{
	local code="${1-1}"
	local errmsg="${2-}"

	if [[ -n "${errmsg}" ]]; then
		>&2 printf '[ERROR]: %s\n' "${errmsg}"
	fi

	return "${code}"
}

## Output API key
#
# @param <1>  Key ID
apikey()
{
	local id="${1-__BAD__}"
	local -A uri=(
		[alphavantage.co]="res/apikey/alphavantage.co.key"
	)
	local path="${uri["${id}"]-__BAD__}"

	[[ "${path}" != '__BAD__' ]] ||  return "${error[bad-args]}"

	cat "${path}"
}

## Run command if stdin is non-empty
#
# @param  <1>    Command
# @params [2..]  Command arguments
ifstdin()
{
	local cmd="${1}"; shift
	local -a args=("${@}")
	local line
	
	IFS= read -r line || true

	if [[ -n "${line}" ]]; then
		( printf '%s\n' "${line}"
		  cat ) \
		| "${cmd}" "${args[@]}"
	fi
}

## Build a reverse hash from an array
#
# Suppose we have the array
#
#     declare -a array=(foo bar baz),
#
# this function produces an array equivalent to
#
#     declare -A rhash=(
#             [foo]=0
#             [bar]=1
#             [baz]=2
#     ).
#
# The reverse hash allows for efficiently fetching the index of an element in
# the original array. Suppose we wish to know the index of 'bar', we can avoid
# iterating over 'array[@]' and just fetch 'rhash[bar]'.
#
# @param <1>  Reverse hash array name
# @param <2>  Data array name
mkrhash()
{
	local rhash="${1}"
	local -n __array__="${2}"

	isdeclared "${rhash}" || return "${error[bad-args]}"

	eval "${rhash}"'=('"$(
		local i
		for i in "${!__array__[@]}"; do
			printf '[%s]=%s\n' "${__array__[i]}" "${i}"
		done
	)"')'
}

## Sort columns of internal CSV data
#
# @param <1>  Hashmap of column names to input order
# @param <2>  Array of sorted column names
# @param <3>  Output column separator
sortcols()
{
	local -n __incol__="${1}"
	local -n __colnames__="${2}"
	local sep="${3}"
	local -a outcols=()
	local colspec=""

	local col
	for col in "${__colnames__[@]}"; do
		outcols+=("${__incol__[${col}]-__EMPTY__}")
	done

	for col in "${outcols[@]}"; do
		if [[ "${col}" == "__EMPTY__" ]]; then
			colspec+="\"\","
		else
			colspec+="\$$((col + 1)),"
		fi
	done
	colspec="${colspec%,}"

	if [[ -n "${colspec}" ]]; then
		ifstdin awk --field-separator "${sep}" "
			BEGIN { OFS=\"${sep}\" }
			{ print(${colspec}) }
		"
	fi
}

## Check if variable exists
#
# @param <1>  Variable name
isdeclared()
{
	local varname="${1}"
	declare -p "${varname}" >/dev/null 2>&1
}


### Vendor-Independent API
#

## Vendor-independent function for fetching time series data
#
# @param <1>  Stock symbol
# @param [2]  Function
# @param [3]  Output size
# @param [4]  Interval
fetch_time_series()
{
	local symbol="${1}"
	local function="${2-default}"
	local outputsize="${3-default}"
	local interval="${4-default}"

	fetch_time_series_alphavantage_co "${symbol}" \
	                                  "${function}" \
	                                  "${outputsize}" \
	                                  "${interval}"
}

## Vendor-independent function for fetching exchange company list
#
# @param <1>  Symbol of stock exchange
fetch_securities()
{
	local symbol="${1}"

	fetch_securities_nasdaq_com "${symbol}"
}


### Vendor-Specific API

## Fetch data from alphavantage.co and process into internal format
#
# @param <1>  Stock symbol
# @param [2]  Function
# @param [3]  Output size
# @param [4]  Interval
fetch_time_series_alphavantage_co()
{
	[[ ${#} -ge 1 ]] || return "${error[bad-args]}"

	local symbol="${1}"
	local function="${2-default}"
	local outputsize="${3-default}"
	local interval="${4-default}"
	local apikey

	apikey="$(apikey alphavantage.co)"

	fetch_raw_time_series_alphavantage_co "${apikey}" \
	                                      "${symbol}" \
	                                      "${function}" \
	                                      "${outputsize}" \
	                                      "${interval}" \
	                                      csv \
	| postfetch_time_series_alphavantage_co
}

## Fetch and postprocess exchange company list data from nasdaq.com
#
# @param <1>  Symbol of exchange
fetch_securities_nasdaq_com()
{
	fetch_raw_securities_nasdaq_com "${@}" \
	| postfetch_securities_nasdaq_com
}


### Fetch and Postprocess Raw Vendor Data
#

## Fetch raw data from alphavantage.co
#
# @param <1>  API key
# @param <2>  Stock symbol
# @param [3]  Function
# @param [4]  Output size
# @param [5]  Interval
# @param [6]  Datatype
fetch_raw_time_series_alphavantage_co()
{
	[[ ${#} -lt 2 ]] && return "${error[bad-args]}"
	[[ -z "${1}" ]] && return "${error[bad-args]}"
	[[ -z "${2}" ]] && return "${error[bad-args]}"

	local url='https://www.alphavantage.co'
	local path="query"
	local query=""

	local -A function=(
		[intraday]='TIME_SERIES_INTRADAY'
		[daily]='TIME_SERIES_DAILY'
		[daily-adjusted]='TIME_SERIES_DAILY_ADJUSTED'
		[weekly]='TIME_SERIES_WEEKLY'
		[monthly]='TIME_SERIES_MONTHLY'
		[default]="${function[daily]}"
	)
	local -A outputsize=(
		[compact]='compact'
		[full]='full'
		[default]="${outputsize[compact]}"
	)
	local -A interval=(
		[1min]='1min'
		[5min]='5min'
		[15min]='15min'
		[30min]='30min'
		[60min]='60min'
		[default]="${interval[60min]}"
	)
	local -A datatype=(
		[json]='json'
		[csv]='csv'
		[default]="${datatype[csv]}"
	)

	local -A field=(
	        [apikey]="${1}"
		[symbol]="${2}"
	        [function]="${function[${3-default}]-__BAD__}"
	        [outputsize]="${outputsize[${4-default}]-__BAD__}"
	        [interval]="${interval[${5-default}]-__BAD__}"
	        [datatype]="${datatype[${6-default}]-__BAD__}"
	)

	local param
	for param in "${!field[@]}"; do
		local value="${field["${param}"]}"

		[[ "${value}" != '__BAD__' ]] || return "${error[bad-args]}"
		query+="&${param}=${value}"
	done
	query="${query#&}"

	curl --silent "${url}/${path}?${query}"
}

## Postprocess alphavantage.co CSV result
postfetch_time_series_alphavantage_co()
{
	local sep="${setting[csv-separator]}"
	# shellcheck disable=SC2034
	local -A rawcols=(
		[id]=0
		[date]=1
		[open]=2
		[high]=3
		[low]=4
		[close]=5
		[volume]=6
	)
	local header
	local errmsg

	IFS= read -r header || true

	if [[ "${header}" == '{' ]]; then
		IFS= read -r errmsg
		errmsg=$( printf '%s' "${errmsg}" \
			 | sed 's/^    "Error Message": "\(.*\)"/\1/' )
		fail "${error[bad-args]}" "${errmsg}"
		return
	fi

	sed "s/\\r$//;
	     s/,/${sep}/g" \
	| tac \
	| nl --number-width=1 --number-separator="${sep}" \
	| sortcols rawcols columns_time_series "${sep}"
}

## Fetch raw company list in stock exchanges from nasdaq.com
#
# @param <1>  Symbol of exchange
fetch_raw_securities_nasdaq_com()
{
	local symbol="${1}"
	local url='http://www.nasdaq.com'
	local path='screening/companies-by-industry.aspx'
	local -A index=(
		[NASDAQ]='NASDAQ'
		[NYSE]='NYSE'
		[AMEX]='AMEX'
	)
	local query='render=download'
	local exchange="${index["${symbol}"]-__BAD__}"

	[[ "${exchange}" != '__BAD__' ]] || return "${error[bad-args]}"
	query+="&exchange=${exchange}"

	curl --silent "${url}/${path}?${query}"
}

## Postprocess nasdaq.com CSV result into internal format
postfetch_securities_nasdaq_com()
{
	local sep="${setting[csv-separator]}"
	# shellcheck disable=SC2034
	local -A rawcols=(
		[id]=0
		[symbol]=1
		[name]=2
		[last-sale]=3
		[cap]=4
		[ipo-year]=5
		[sector]=6
		[industry]=7
		[quote]=8
	)

	IFS= read -r || true
	sed 's/^"//;
	     s/",\r//;
	     s/","/'"${sep}"'/g;' \
	| nl --number-width=1 --number-separator="${sep}" \
	| sortcols rawcols columns_securities_list "${sep}"
}


### Data Mungers
#

## Filter column
#
# @params <1..>  Columns to preserve
munge_filter_columns()
{
	local -a cols=("${@}")
	local sep="${setting[csv-separator]}"
	local fields=""

	local i
	for i in "${!cols[@]}"; do
		cols[i]=$((cols[i] + 1))
	done

	fields="$(IFS=','; printf '%s' "${cols[*]}")"
	[[ -z "${fields}" ]] && return

	cut --delimiter="${sep}" \
	    --output-delimiter="${sep}" \
	    --fields="${fields}"
}

## Prepend metadata header
#
# We generate minmax data for specified input columns and organize into a
# header which we prepend to the input data, separated by a newline. The output
# format is as follows:
#
#     <num data cols><sep><num data rows>
#     <arg #1 col min><sep><arg #1 col max>
#     <arg #2 col min><sep><arg #2 col max>
#     ...
#     <<newline>>
#     <input data>
#
# If no arguments are given, then the header will consist of just the first row
# above.
#
# @params [1..]  Columns to operate on
munge_metadata_header()
{
	local -a hcols=("${@}")
	local sep="${setting[csv-separator]}"
	local ncols nrows
	local -a fields mins maxs
	local tmp

	tmp=$(mktemp)
	trap 'rm '"${tmp}" EXIT

	IFS="${sep}" read -ra fields
	ncols=${#fields[@]}
	nrows=1
	mins=( "${fields[@]}" )
	maxs=( "${fields[@]}" )

	( IFS="${sep}"
	  printf '%s\n' "${fields[*]}"
	) >>"${tmp}"

	while IFS="${sep}" read -ra fields; do
		nrows=$((nrows + 1))

		local i
		for i in "${hcols[@]}"; do
			local f="${fields[i]}"
			local min="${mins[i]}"
			local max="${maxs[i]}"

			mins[i]=$(dc --expression="[${f}]sM ${min}d ${f} <Mp")
			maxs[i]=$(dc --expression="[${f}]sM ${max}d ${f} >Mp")
		done

		( IFS="${sep}"
		  printf '%s\n' "${fields[*]}"
		) >>"${tmp}"
	done

	( IFS="${sep}"
	  printf '%s%s%s\n' "${ncols}" "${sep}" "${nrows}"

	  local i
	  for i in "${hcols[@]}"; do
		printf '%s%s%s\n' "${mins[i]}" "${sep}" "${maxs[i]}"
	  done

	  printf '\n'
	  cat
	) <"${tmp}"
}

## Generate EMA(s) from data column
#
# EMA stands for ``exponential moving average''. Here we generate this for a
# specified column of input data. The output row format is as follows:
#
#     <id><sep><weight><sep><ema>
#
# The <weight> column allows us to unambiguously generate data for multiple
# weights. A simple use case would be to view a few overlayed EMA curves.
# However, a more sophisticated use might be to generate EMAs for an entire
# interval of weights in order to produce a 3D graph of an EMA ``sheet''.
#
# NOTE: It is useful to note the extreme edge cases of weight values W:
#
#    1) when W = 0, the EMA is equal to original data, and
#    2) when W = 1, the EMA is equal to the (moving) arithmetic mean.
#
# @param  <1>    ID column
# @param  <2>    Data column
# @params <3..>  Exponential weight(s)
munge_multi_ema()
{
	local idcol="${1}"
	local datcol="${2}"
	local sep="${setting[csv-separator]}"

	shift 2
	local -a weight=("${@}")

	local w
	for w in "${weight[@]}"; do
		if [[ $(bc <<<"${w} <  0") -eq 1 ]] \
		|| [[ $(bc <<<"${w} >  1") -eq 1 ]]; then
			return "${error[bad-arg]}"
		fi
	done

	local -a fields
	IFS="${sep}" read -ra fields

	local -a sum=()
	local -a meas=()
	for w in "${weight[@]}"; do
		sum+=("${fields[${datcol}]}")
		meas+=(1)
		printf '%s%s%s%s%s\n' "${fields[${idcol}]}" \
		             "${sep}" "${w}" \
		             "${sep}" "${fields[${datcol}]}"
	done

	while IFS="${sep}" read -ra fields; do
		local -a ema=()
		local datum="${fields[${datcol}]}"
		local bcmeas=""
		local bcsum=""
		local bcema=""

		local i
		for i in "${!weight[@]}"; do
			bcmeas+="${meas[i]} * ${weight[i]} + 1;"
			bcsum+="${sum[i]} * ${weight[i]} + ${datum};"
			bcema+="${sum[i]}/${meas[i]};"
		done

		meas=( $(bc <<<"${bcmeas}") )
		sum=( $(bc <<<"${bcsum}") )
		ema=( $(bc <<<"${bcema}") )

		for i in "${!ema[@]}"; do
			printf '%s%s%s%s%s\n' "${fields[${idcol}]}" \
			             "${sep}" "${weight[i]}" \
			             "${sep}" "${ema[i]}"
		done
	done
}


### Data Visualization Functions
#

## Candlestick plot
#
# @param <1>  Plot x-axis column
# @param <2>  Plot open column
# @param <3>  Plot low column
# @param <4>  Plot high column
# @param <5>  Plot close column
plot_candlestick()
{
	local xcol="${1}"
	local open="${2}"
	local low="${3}"
	local high="${4}"
	local close="${5}"
	local negcolor="${setting[plot-candlestick-neg-color]}"
	local poscolor="${setting[plot-candlestick-pos-color]}"
	local pad="${setting[plot-vert-padding]}"
	local sep="${setting[csv-separator]}"
	local negval=-1
	local posval=1
	local xmin xmax ymin ymax

	IFS= read -r  # ncols nrows
	IFS="${sep}" read -r xmin xmax
	IFS="${sep}" read -r ymin _
	IFS="${sep}" read -r _ ymax
	IFS= read -r

	local vrange
	vrange=$(bc <<<"${ymax} - ${ymin}")
	ymin=$(bc <<<"${ymin} - ${vrange} * ${pad}")
	ymax=$(bc <<<"${ymax} + ${vrange} * ${pad}")

	ifstdin gnuplot --persist -e "
		set datafile separator '${sep}';

		set xrange [${xmin}:${xmax}];
		set yrange [${ymin}:${ymax}];

		set palette defined ( ${negval} '${negcolor}'
		                    , ${posval} '${poscolor}' );
		set cbrange [${negval}:${posval}];
		unset colorbox;
		set style fill solid noborder;

		plot '<cat' using $((xcol + 1))
		                  :$((open + 1))
		                  :$((low + 1))
		                  :$((high + 1))
		                  :$((close + 1))
		                  :(\$$((close + 1)) < \$$((open + 1))
		                    ? ${negval}
		                    : ${posval})
		            with candlesticks
		            palette
		            notitle;
	"
}

## Simple line plot
#
# @param <1>  Plot x-axis column
# @param <2>  Plot y-axis column
plot_linespoints()
{
	local xcol="${1}"
	local ycol="${2}"
	local pad="${setting[plot-vert-padding]}"
	local sep="${setting[csv-separator]}"
	local xmin xmax ymin ymax

	IFS= read -r  # ncols nrows
	IFS="${sep}" read -r xmin xmax
	IFS="${sep}" read -r ymin ymax
	IFS= read -r

	local vrange
	vrange=$(bc <<<"${ymax} - ${ymin}")
	ymin=$(bc <<<"${ymin} - ${vrange} * ${pad}")
	ymax=$(bc <<<"${ymax} + ${vrange} * ${pad}")

	ifstdin gnuplot --persist -e "
		set datafile separator '${sep}';

		set xrange [${xmin}:${xmax}];
		set yrange [${ymin}:${ymax}];

		plot '<cat' using $((xcol + 1)) : $((ycol + 1))
		            with linespoints
		            notitle;
	"
}

## 3D surface plot
#
# @param <1>  Plot x-axis column
# @param <2>  Plot y-axis column
# @param <3>  Plot z-axis column
plot_dgrid3d()
{
	local xcol="${1}"
	local ycol="${2}"
	local zcol="${3}"
	local meshsz="${setting[plot-dgrid3d-mesh-size]}"
	local sep="${setting[csv-separator]}"
	local dat

	dat=$(mktemp)
	trap 'rm '"${dat}" EXIT
	cat >"${dat}"

	gnuplot --persist -e "
		set datafile separator '${sep}';
		set dgrid3d ${meshsz}, ${meshsz};

		splot '${dat}' using $((xcol + 1))
		                      :$((ycol + 1))
		                      :$((zcol + 1))
		                      with lines
		                      notitle;

		pause mouse close
	"
}


### Command Functions
#

cmd_list_securities()
{
	fetch_securities "${@}" \
	| munge_filter_columns "${_sl[symbol]}" "${_sl[name]}"
}

cmd_list_time_series()
{
	fetch_time_series "${@}"
}

cmd_plot_time_series()
{
	fetch_time_series "${@}" \
	| munge_metadata_header "${_ts[id]}" "${_ts[low]}" "${_ts[high]}" \
	| plot_candlestick "${_ts[id]}" \
	                   "${_ts[open]}" \
	                   "${_ts[low]}" \
	                   "${_ts[high]}" \
	                   "${_ts[close]}"
}

cmd_plot_ema()
{
	fetch_time_series "${@}" \
	| munge_multi_ema "${_ts[id]}" "${_ts[close]}" \
	                  "${setting[ema-weight]}" \
	| munge_metadata_header "${_me[id]}" "${_me[ema]}" \
	| plot_linespoints "${_me[id]}" "${_me[ema]}"
}

cmd_plot_ema_interval()
{
	local n="${setting[ema-bin-count]}"
	local max="${setting[ema-max]}"
	local min="${setting[ema-min]}"
	local -a weights=()
	local d

	d=$(bc <<<"(${max} - ${min})/${n}")

	local i
	for i in $(seq 0 "${n}"); do
		local w
		w=$(bc <<<"${min} + ${i}*${d}")
		weights+=("${w}")
	done

	fetch_time_series "${@}" \
	| munge_multi_ema "${_ts[id]}" "${_ts[close]}" "${weights[@]}" \
	| plot_dgrid3d "${_me[id]}" "${_me[weight]}" "${_me[ema]}"
}

usage()
{
	cat <<- EOF
	Usage: $(basename "${0}") <command> <command args>

	Commands

	    list-secs <exchange symbol>
	    list-ts <symbol> [function] [output size] [interval]
	    plot-ts <symbol> [function] [output size] [interval]
	    plot-ema <symbol> [function] [output size] [interval]
	    plot-ema-int <symbol> [function] [output size] [interval]

	Command Args

	    function = < intraday | daily | daily-adjusted | weekly | monthly >
	    outputsize = < compact | full >
	    interval = < 1min | 5min | 15min | 30min | 60min >

	EOF
}


### Main UI and Options Processing
#

main()
{
	if [[ ${#} -eq 0 ]]; then
		usage
		return "${error[bad-cmd]}"
	fi

	local cmd="${1}"
	shift

	case "${cmd}" in
		list-secs) cmd_list_securities "${@}";;
		list-ts) cmd_list_time_series "${@}";;
		plot-ts) cmd_plot_time_series "${@}";;
		plot-ema) cmd_plot_ema "${@}";;
		plot-ema-int) cmd_plot_ema_interval "${@}";;
		_debug) "${@}";;
		*) usage;;
	esac
}


mkrhash _ts columns_time_series
mkrhash _me columns_multi_ema
mkrhash _sl columns_securities_list


if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
	main "${@}"
fi
